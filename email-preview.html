<!DOCTYPE html><html><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>HDC Daily Digest – Preview</title></head><body style="font-family: system-ui, sans-serif; max-width: 720px; margin: 0 auto; padding: 1rem;"><h1>HDC Daily Digest</h1><p>Date: 2026-02-14</p><h2>Papers</h2><h3><a href="https://www.frontiersin.org/articles/10.3389/frai.2026.1690492/full">Optimal hyperdimensional representation for learning and cognitive computation</a></h3>
              <p><b>Published:</b> 2026-02-10 | Frontiers in Artificial Intelligence</p>
              <p>This peer-reviewed article proposes a universal hyperdimensional encoding method that adapts representations to the differing needs of learning (correlated representations) and cognitive computation (orthogonal, separable representations). The authors derive a separation metric to quantify the trade-off and validate the approach empirically on image classification and decoding tasks, reporting large improvements in accuracy when tuning encoder correlation. The paper situates the results within HDC/VSA theory and discusses implications for encoder design in learning vs. reasoning systems.</p>
              <hr/><h3><a href="https://arxiv.org/abs/2601.02509">hdlib 2.0: Extending Machine Learning Capabilities of Vector-Symbolic Architectures</a></h3>
              <p><b>Published:</b> 2026-01-05 | arXiv</p>
              <p>This arXiv preprint presents hdlib 2.0, a major update to a Python library for designing Vector-Symbolic Architectures (VSA)/Hyperdimensional Computing. The release adds supervised classification enhancements (including feature selection), new regression, clustering and graph-based learning models, and a proposed implementation path for Quantum Hyperdimensional Computing. The authors provide links to the open-source repository and documentation for reproducing experiments and using the new models.</p>
              <hr/><h3><a href="https://arxiv.org/abs/2601.20061">Primitive-Driven Acceleration of Hyperdimensional Computing for Real-Time Image Classification</a></h3>
              <p><b>Published:</b> 2026-01-27 | arXiv</p>
              <p>This arXiv paper develops an image-encoding algorithm that maps local image patches to hypervectors and merges them using HDC operations, achieving improved classification accuracy on MNIST and Fashion-MNIST compared to prior HDC encoders. The work also designs an FPGA accelerator (Alveo U280) implementing binding, permutation, bundling and similarity search with a pipelined architecture, reporting sub-millisecond inference latency and large speedups vs. CPU/GPU baselines. The paper focuses on algorithm-architecture co-design to enable real-time HDC image classification.</p>
              <hr/><h3><a href="https://arxiv.org/abs/2511.12664">Quantum Hyperdimensional Computing: a foundational paradigm for quantum neuromorphic architectures</a></h3>
              <p><b>Published:</b> 2025-11-16 | arXiv</p>
              <p>This arXiv preprint introduces Quantum Hyperdimensional Computing (QHDC), mapping classical HDC operations onto quantum-native primitives: hypervectors to quantum states, bundling via linear combination of unitaries and amplitude amplification, binding via phase oracles, and permutation via QFT. The authors report implementations and comparisons across classical, simulated quantum, and an IBM quantum processor, arguing that QHDC offers resource-efficient mappings and potential for quantum neuromorphic algorithms. The paper includes experiments on symbolic reasoning and supervised classification to validate the framework.</p>
              <hr/><h3><a href="https://arxiv.org/abs/2511.03911">DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets</a></h3>
              <p><b>Published:</b> 2025-11-05 | arXiv</p>
              <p>DecoHD proposes a decomposed parameterization for HDC that learns a small set of shared channels with multiplicative binding across layers and a bundling head, enabling very compact models while preserving HDC inference as pure hypervector operations. The method achieves large memory savings (up to ~97% fewer trainable parameters) with minor accuracy degradation and demonstrates increased robustness to bit-flip noise. The authors evaluate memory, accuracy, and hardware energy/speed trade-offs against CPU/GPU/ASIC baselines.</p>
              <hr/><h3><a href="https://academic.oup.com/bioinformatics/article/40/7/btae452/7714688">HyperGen: compact and efficient genome sketching using hyperdimensional vectors</a></h3>
              <p><b>Published:</b> 2024-07-16 | Bioinformatics (Oxford Academic)</p>
              <p>HyperGen (Bioinformatics, July 16, 2024) applies hyperdimensional computing to genome sketching for Average Nucleotide Identity (ANI) estimation, encoding genomes as quasi-orthogonal hypervectors to produce compact sketches. The paper reports comparable or superior ANI estimation accuracy and faster sketching/search runtimes than existing sketch-based methods and provides an open-source Rust implementation. The approach leverages vector-multiply operations to benefit from optimized GEMM routines for large-scale genomic databases.</p>
              <hr/><h3><a href="https://arxiv.org/abs/2311.08150">The Hyperdimensional Transform for Distributional Modelling, Regression and Classification</a></h3>
              <p><b>Published:</b> 2023-11-14 | arXiv</p>
              <p>This arXiv paper introduces the hyperdimensional transform as a theoretical foundation for representing functions and distributions as high-dimensional holographic vectors, and shows how it unifies and extends HDC methods for regression, classification, and statistical modelling. The authors present algorithmic modifications, sampling and Bayesian inference techniques in the HDC framework and demonstrate use cases across typical machine learning tasks. The work aims to provide a principled toolbox connecting HDC with broader data science methodologies.</p>
              <hr/><h3><a href="https://arxiv.org/abs/2004.11204">Classification using Hyperdimensional Computing: A Review</a></h3>
              <p><b>Published:</b> 2020-04-19 | arXiv</p>
              <p>This survey reviews hyperdimensional computing (HDC) methods for classification, covering hypervector data representation, encoding, transformation operations (addition, multiplication, permutation), similarity measures, and strategies for accuracy-efficiency trade-offs such as binarization and hardware acceleration. The authors summarize applications across letters, signals and images and discuss HDC&#x27;s robustness, fast learning and energy-efficiency properties, positioning HDC as a lightweight classifier for IoT and edge scenarios. The review synthesizes prior work and identifies common encoding and retraining approaches.</p>
              <hr/><h2>News</h2><h3><a href="https://www.sciencedirect.com/science/article/pii/S2213846326000040">Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment</a></h3>
              <p><b>Published:</b> 2026-02-09 | Manufacturing Letters / Elsevier</p>
              <p>A paper available online 9 February 2026 in Manufacturing Letters compares hyperdimensional computing (HDC) with conventional AI models for in-situ sensing-based prediction of geometric quality in CNC machining. The authors report that HDC achieves comparable accuracy while dramatically lowering energy consumption (claimed ~200× for training and 175–1000× for inference) and shortening training/inference times by hundreds of times. The study frames HDC as a promising energy-efficient alternative for smart manufacturing use cases.</p>
              <hr/><h3><a href="https://ercim-news.ercim.eu/en125/special/fulfilling-brain-inspired-hyperdimensional-computing-with-in-memory-computing">Fulfilling Brain-inspired Hyperdimensional Computing with In-memory Computing</a></h3>
              <p><b>Published:</b> 2026-02-02 | ERCIM News</p>
              <p>An ERCIM News special-theme article (Feb 2, 2026) by researchers at IBM Research Europe describes an end-to-end in-memory implementation of hyperdimensional computing (HDC) on memristive crossbar arrays. The article summarizes energy-efficiency and robustness benefits demonstrated across classification tasks (language recognition, news classification, hand-gesture recognition) and discusses mapping HDC primitives (binding, bundling, permutation) to noisy in-memory hardware. It positions in-memory HDC as a route toward low-energy, brain-inspired edge computing.</p>
              <hr/><h3><a href="https://arxiv.org/abs/2601.20061">Primitive-Driven Acceleration of Hyperdimensional Computing for Real-Time Image Classification</a></h3>
              <p><b>Published:</b> 2026-01-27 | arXiv</p>
              <p>ArXiv preprint (Jan 27, 2026) presenting an image-encoding algorithm and an FPGA accelerator for hyperdimensional computing (HDC). The authors propose patch-level hypervector encodings merged via HDC operations and report strong accuracy on MNIST/Fashion-MNIST, plus an FPGA implementation (Alveo U280) delivering 0.09 ms inference latency and large speedups versus CPU/GPU baselines. The paper focuses on optimizing HDC primitives (binding, permutation, bundling, similarity) for real-time image tasks.</p>
              <hr/><h3><a href="https://arxiv.org/abs/2511.12664">Quantum Hyperdimensional Computing: a foundational paradigm for quantum neuromorphic architectures</a></h3>
              <p><b>Published:</b> 2025-11-16 | arXiv</p>
              <p>An arXiv preprint (Nov 16, 2025) proposing &#x27;Quantum Hyperdimensional Computing&#x27; (QHDC) that maps classical HDC operations to quantum-native procedures (e.g., hypervectors→quantum states, bundling→LCU/OAA, permutation→QFT). The authors demonstrate implementations in simulation and on an IBM quantum processor, arguing that HDC maps naturally to quantum hardware and could enable quantum neuromorphic algorithms for reasoning and classification. The work frames QHDC as a roadmap for quantum-native cognitive computing.</p>
              <hr/><h3><a href="https://arxiv.org/abs/2508.12021">FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing</a></h3>
              <p><b>Published:</b> 2025-08-16 | arXiv</p>
              <p>ArXiv preprint (Aug 16, 2025) introducing FedUHD, a federated unsupervised learning framework built on hyperdimensional computing (HDC). The authors describe client-side HDC designs (including kNN-based cluster hypervector removal) and a weighted HDC aggregation at the server to address non-iid data, reporting large speed and energy gains as well as improved robustness compared to neural-network baselines. The paper highlights HDC&#x27;s suitability for resource-constrained, privacy-preserving edge/federated settings.</p>
              <hr/><h3><a href="https://arxiv.org/abs/2510.23980">HyperGraphX: Graph Transductive Learning with Hyperdimensional Computing and Message Passing</a></h3>
              <p><b>Published:</b> 2025-10-28 | arXiv</p>
              <p>ArXiv preprint (Oct 28, 2025) that combines graph message passing with hyperdimensional computing (HDC) operations to form a transductive learning method called HDGC. The authors report accuracy gains over graph neural networks and HDC baselines on several graph tasks and highlight orders-of-magnitude speedups on GPUs, arguing binary-vector HDC operations can yield energy-efficient graph learning on neuromorphic or PIM hardware.</p>
              <hr/><h3><a href="https://communities.springernature.com/posts/call-for-papers-hyperdimensional-computing-and-vector-symbolic-architectures">Call for papers: Hyperdimensional Computing and Vector Symbolic Architectures</a></h3>
              <p><b>Published:</b> 2025-07-31 | Springer Nature</p>
              <p>A Springer Nature &#x27;call for papers&#x27; (posted Jul 31, 2025) announcing a collection soliciting theoretical and practical advances in Hyperdimensional Computing (HD) / Vector Symbolic Architectures (VSA). The collection invites submissions across disciplines (AI/ML, neuroscience, hardware) with a submission deadline of 16 March 2026, signaling growing mainstream publisher interest in HD/VSA topics.</p>
              <hr/><h3><a href="https://www.hd-computing.com/events">Events — HD/VSA (community page listing workshops, deadlines and webinars)</a></h3>
              <p><b>Published:</b> — | hd-computing.com</p>
              <p>Community events page for the Hyperdimensional Computing / Vector Symbolic Architectures (HD/VSA) community listing upcoming webinars, deadlines (e.g., a Jan 31, 2026 paper deadline referenced on the page), and conference/workshop information. The page aggregates community announcements and resources (benchmarks, datasets, webinars) relevant to researchers and practitioners in HDC/VSA.</p>
              <hr/><h2>Blogs</h2><h3><a href="https://research.ibm.com/blog/in-memory-hyperdimensional-computing">Fulfilling brain-inspired hyperdimensional computing with in-memory computing</a></h3>
              <p><b>Published:</b> 2020-06-01 | IBM Research</p>
              <p>An IBM Research blog post describing an end-to-end prototype that maps hyperdimensional computing (HDC) primitives to an in-memory computing architecture. It explains HDC concepts (hypervectors, binding, bundling, permutation), presents experiments on language/news/gesture tasks, and argues that HDC is a promising workload for energy-efficient in-memory hardware. The post links the prototype to a Nature Electronics paper and lists collaborators and implementation details.</p>
              <hr/><h3><a href="https://www.hd-computing.com/">HD/VSA</a></h3>
              <p><b>Published:</b> — | HD/VSA (hd-computing.com)</p>
              <p>A community-maintained site introducing Hyperdimensional Computing / Vector Symbolic Architectures (HD/VSA). The site collects motivation, origins, models (HRR, BSC, MAP), examples, and links to publications, software, and community resources for researchers and practitioners. It provides tutorial-style explanations of binding, bundling, permutation, and representative implementations.</p>
              <hr/><h3><a href="https://www.emergentmind.com/topics/hyperdimensional-computing-hdc-3551e854-d144-4fab-8414-226881d6ccb4">HyperDimensional Computing (HDC)</a></h3>
              <p><b>Published:</b> — | EmergentMind</p>
              <p>An EmergentMind topic/overview page that summarizes principles, operations (binding, bundling, permutation), hardware mappings (in-memory, photonics), and recent research directions in HDC. The page gives a practical overview of algorithmic workflows, dimensionality/robustness trade-offs, and applications such as graphs, vision, and bioinformatics, with links to relevant papers and implementations.</p>
              <hr/><h3><a href="https://medium.com/stanford-cs224w/hyperdimensional-computing-for-graphs-machine-learning-56b381ebbc27">Hyperdimensional Computing for Graphs Machine Learning</a></h3>
              <p><b>Published:</b> 2024-10-07 | Medium (Stanford CS224W)</p>
              <p>A Stanford CS224W Medium tutorial showing how to apply HDC to graph learning tasks. It explains hypervector representations (binary spatter codes), binding and bundling for edges and nodes, and includes PyTorch code and experiments on the Cora dataset. The post is a practical walk-through aimed at students and engineers exploring lightweight alternatives to GNNs using HDC.</p>
              <hr/><h3><a href="https://rabmcmenemy.medium.com/building-an-enhanced-hyperdimensional-computing-kolmogrov-arnold-neurosymbolic-model-with-pytorch-99b528b29505">Building An Enhanced Hyperdimensional Computing-Kolmogrov Arnold-Neurosymbolic Model with PyTorch</a></h3>
              <p><b>Published:</b> 2025-01-08 | Medium (Robert McMenemy)</p>
              <p>A Medium blog post by Robert McMenemy describing an experimental integration of hyperdimensional computing (HDC) with Kolmogorov–Arnold networks and neurosymbolic modules implemented in PyTorch. The article outlines the architecture, mathematical foundations, and provides implementation details and code snippets aimed at practical model building and reproducibility. It emphasizes interpretability and robustness benefits from combining HDC with symbolic/neural components.</p>
              <hr/><h3><a href="https://rabmcmenemy.medium.com/harnessing-hyperdimensional-computing-for-cpu-operation-classification-724c60690b32">Harnessing Hyperdimensional Computing for CPU Operation Classification</a></h3>
              <p><b>Published:</b> 2025-01-28 | Medium (Robert McMenemy)</p>
              <p>A Medium project post by Robert McMenemy that applies HDC to classify CPU operations (e.g., memory vs arithmetic). The post presents the problem setup, the HDC encoding strategy (hypervectors, binding/bundling), implementation details, and evaluation results, with code excerpts for reproduction. It is pitched as a technical how-to for using HDC in systems and performance-analysis tasks.</p>
              <hr/><h3><a href="https://medium.com/%40zendehdel.d/graphhd-445360cd1831">GraphHD — Exploring GraphHD: A HyperDimensional Computing Approach to Graph Classification</a></h3>
              <p><b>Published:</b> 2024-05-09 | Medium (Zendehdel D)</p>
              <p>A concise Medium post that explains GraphHD, an HDC-based approach for graph classification. The article describes how GraphHD encodes graph structure into hypervectors via binding and bundling of edges and nodes, contrasts the method with GNNs, and summarizes efficiency and robustness trade-offs that make GraphHD attractive for resource-constrained scenarios. It provides a high-level walkthrough intended for practitioners interested in lightweight graph classifiers.</p>
              <hr/></body></html>